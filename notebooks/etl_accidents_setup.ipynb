{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a3e5bda",
   "metadata": {},
   "source": [
    "\n",
    "# 🧭 Accidents — Notebook unique (Setup → Ingestion → Modélisation → Analyse)\n",
    "\n",
    "Ce notebook **centralise tout** pour l'équipe :  \n",
    "1) **Setup PostgreSQL local** (création base/utilisateur schémas)  \n",
    "2) **Ingestion** depuis l'API publique (Opendatasoft) vers **BRONZE**  \n",
    "3) **Modélisation** (SILVER / GOLD)  \n",
    "4) **Analyses SQL** (requêtes prêtes à l'emploi)  \n",
    "\n",
    "> Hypothèse : chacun a **PostgreSQL installé en local** (port 5432).  \n",
    "> On évite Docker. DBeaver reste facultatif (client), **tout s'exécute ici**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35273820",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Pré-requis Python (une fois)\n",
    "Exécute la cellule suivante si besoin pour installer les dépendances dans ton environnement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c20add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Si nécessaire, décommente la ligne suivante :\n",
    "# %pip install psycopg2-binary SQLAlchemy pandas python-dotenv requests tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482afbd2",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Configuration (variables d'environnement)\n",
    "- Par défaut on utilise `accidents/accidents` sur `localhost:5432` et base `accidents`.\n",
    "- Si tu veux surcharger, crée un fichier `.env` à côté de ce notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b210445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a minimal notebook to: Create/Connect DB → Create bronze → Insert from the given CSV path.\n",
    "import nbformat as nbf\n",
    "from pathlib import Path\n",
    "\n",
    "nb = nbf.v4.new_notebook()\n",
    "cells = []\n",
    "\n",
    "cells.append(nbf.v4.new_markdown_cell(\n",
    "\"# 🟤 Bronze — Ingestion CSV unique (PostgreSQL local)\\n\\n\"\n",
    "\"Scope **strict** :\\n\"\n",
    "\"1. Créer/Connecter la base locale\\n\"\n",
    "\"2. Créer le schéma `bronze` et la table de stockage brut\\n\"\n",
    "\"3. **Charger UNIQUEMENT** le fichier CSV fourni (une passe)\\n\\n\"\n",
    "\"> Hypothèses : PostgreSQL installé en local (port 5432). Pas de Docker requis.\\n\"\n",
    "\"> Tout s'exécute **ici**, DBeaver facultatif.\\n\"\n",
    "))\n",
    "\n",
    "cells.append(nbf.v4.new_markdown_cell(\"## 0) Dépendances (exécuter une seule fois si besoin)\"))\n",
    "cells.append(nbf.v4.new_code_cell(\n",
    "\"# Décommente si nécessaire :\\n\"\n",
    "\"# %pip install psycopg2-binary SQLAlchemy pandas python-dotenv pyarrow\\n\"\n",
    "))\n",
    "\n",
    "cells.append(nbf.v4.new_markdown_cell(\n",
    "\"## 1) Configuration\\n\"\n",
    "\"- Modifiez `RAW_FILE` si nécessaire. Par défaut :\\n\"\n",
    "\"  - `etl-road-safety\\\\data\\\\cleaned\\\\accidents_clean.csv` (Windows)\\n\"\n",
    "\"  - fallback: `./data/raw/accidents_clean.csv`\\n\"\n",
    "\"  - fallback: `/mnt/data/accidents_clean.csv` (pour test ici)\\n\"\n",
    "\"- Vous pouvez surcharger la connexion via un `.env`.\\n\"\n",
    "))\n",
    "cells.append(nbf.v4.new_code_cell(\n",
    "\"import os\\n\"\n",
    "\"from pathlib import Path\\n\"\n",
    "\"from dotenv import load_dotenv\\n\\n\"\n",
    "\"# Charger .env si présent (facultatif)\\n\"\n",
    "\"if Path('.env').exists():\\n\"\n",
    "\"    load_dotenv('.env')\\n\\n\"\n",
    "\"# Connexion PostgreSQL locale (par défaut)\\n\"\n",
    "\"PG_HOST = os.getenv('POSTGRES_HOST', 'localhost')\\n\"\n",
    "\"PG_PORT = int(os.getenv('POSTGRES_PORT', '5432'))\\n\"\n",
    "\"PG_DB   = os.getenv('POSTGRES_DB',   'accidents')\\n\"\n",
    "\"PG_USER = os.getenv('POSTGRES_USER', 'accidents')\\n\"\n",
    "\"PG_PASS = os.getenv('POSTGRES_PASSWORD', 'accidents')\\n\"\n",
    "\"PG_SU_USER = os.getenv('POSTGRES_SU_USER', 'postgres')\\n\"\n",
    "\"PG_SU_PASS = os.getenv('POSTGRES_SU_PASSWORD', 'postgres')\\n\\n\"\n",
    "\"# Fichier CSV cible (ordre de priorité)\\n\"\n",
    "\"candidates = [\\n\"\n",
    "\"    Path(r'etl-road-safety\\\\data\\\\cleaned\\\\accidents_clean.csv'),\\n\"\n",
    "\"    Path('./data/raw/accidents_clean.csv'),\\n\"\n",
    "\"    Path('/mnt/data/accidents_clean.csv'),\\n\"\n",
    "\"]\\n\"\n",
    "\"RAW_FILE = next((p for p in candidates if p.exists()), candidates[0])\\n\"\n",
    "\"print('🔧 Config DB:', f'{PG_USER}@{PG_HOST}:{PG_PORT}/{PG_DB}')\\n\"\n",
    "\"print('📄 Fichier CSV:', RAW_FILE)\\n\"\n",
    "))\n",
    "\n",
    "cells.append(nbf.v4.new_markdown_cell(\"## 2) Connexion & Bootstrap (idempotent)\"))\n",
    "cells.append(nbf.v4.new_code_cell(\n",
    "\"from sqlalchemy import create_engine, text\\n\"\n",
    "\"from sqlalchemy.engine import Engine\\n\\n\"\n",
    "\"def mk_url(user, pwd, db):\\n\"\n",
    "\"    return f'postgresql+psycopg2://{user}:{pwd}@{PG_HOST}:{PG_PORT}/{db}'\\n\\n\"\n",
    "\"def get_engine(user, pwd, db) -> Engine:\\n\"\n",
    "\"    return create_engine(mk_url(user, pwd, db), pool_pre_ping=True, future=True)\\n\\n\"\n",
    "\"def run_sql(engine: Engine, sql: str, params: dict|None=None):\\n\"\n",
    "\"    with engine.begin() as conn:\\n\"\n",
    "\"        conn.execute(text(sql), params or {})\\n\\n\"\n",
    "\"# Superuser → créer rôle et base si absents\\n\"\n",
    "\"su_engine = get_engine(PG_SU_USER, PG_SU_PASS, 'postgres')\\n\"\n",
    "\"run_sql(su_engine, f\\\"\\\"\\\"\\n\"\n",
    "\"DO $$\\n\"\n",
    "\"BEGIN\\n\"\n",
    "\"   IF NOT EXISTS (SELECT FROM pg_roles WHERE rolname = '{PG_USER}') THEN\\n\"\n",
    "\"      CREATE ROLE {PG_USER} LOGIN PASSWORD '{PG_PASS}';\\n\"\n",
    "\"   END IF;\\n\"\n",
    "\"END$$;\\n\"\n",
    "\"\\\"\\\"\\\")\\n\"\n",
    "\"run_sql(su_engine, f\\\"\\\"\\\"\\n\"\n",
    "\"DO $$\\n\"\n",
    "\"BEGIN\\n\"\n",
    "\"   IF NOT EXISTS (SELECT FROM pg_database WHERE datname = '{PG_DB}') THEN\\n\"\n",
    "\"      CREATE DATABASE {PG_DB} OWNER {PG_USER};\\n\"\n",
    "\"   END IF;\\n\"\n",
    "\"END$$;\\n\"\n",
    "\"\\\"\\\"\\\")\\n\\n\"\n",
    "\"# Connexion applicative et schéma bronze\\n\"\n",
    "\"app_engine = get_engine(PG_USER, PG_PASS, PG_DB)\\n\"\n",
    "\"run_sql(app_engine, 'CREATE SCHEMA IF NOT EXISTS bronze AUTHORIZATION current_user;')\\n\"\n",
    "\"print('✅ Bootstrap OK — base, user, schéma bronze prêts.')\\n\"\n",
    "))\n",
    "\n",
    "cells.append(nbf.v4.new_markdown_cell(\"## 3) Table BRONZE (stockage brut JSONB)\"))\n",
    "cells.append(nbf.v4.new_code_cell(\n",
    "\"bronze_sql = (\\n\"\n",
    "\"    'CREATE TABLE IF NOT EXISTS bronze.raw_ingest ('\\n\"\n",
    "\"    '  id_big BIGSERIAL PRIMARY KEY,'\\n\"\n",
    "\"    '  source_file TEXT,'\\n\"\n",
    "\"    '  payload_json JSONB NOT NULL,'\\n\"\n",
    "\"    '  ingested_at TIMESTAMPTZ NOT NULL DEFAULT NOW()'\\n\"\n",
    "\"    ');'\\n\"\n",
    "\"    'CREATE INDEX IF NOT EXISTS ix_raw_payload_gin ON bronze.raw_ingest USING GIN (payload_json);'\\n\"\n",
    "\")\\n\"\n",
    "\"run_sql(app_engine, bronze_sql)\\n\"\n",
    "\"print('✅ Table bronze.raw_ingest prête.')\\n\"\n",
    "))\n",
    "\n",
    "cells.append(nbf.v4.new_markdown_cell(\"## 4) Chargement du CSV → bronze.raw_ingest\"))\n",
    "cells.append(nbf.v4.new_code_cell(\n",
    "\"import pandas as pd\\n\"\n",
    "\"import json\\n\"\n",
    "\"from sqlalchemy import text\\n\"\n",
    "\"from pathlib import Path\\n\\n\"\n",
    "\"assert Path(RAW_FILE).exists(), f'Fichier introuvable: {RAW_FILE}'\\n\"\n",
    "\"try:\\n\"\n",
    "\"    df = pd.read_csv(RAW_FILE, sep=';', dtype=str, low_memory=False)\\n\"\n",
    "\"except Exception:\\n\"\n",
    "\"    df = pd.read_csv(RAW_FILE, sep=',', dtype=str, low_memory=False)\\n\"\n",
    "\"df = df.fillna(value=None)\\n\"\n",
    "\"records = df.to_dict(orient='records')\\n\"\n",
    "\"print('📦 Lignes à insérer:', len(records))\\n\\n\"\n",
    "\"batch_size = 5000\\n\"\n",
    "\"total = 0\\n\"\n",
    "\"for i in range(0, len(records), batch_size):\\n\"\n",
    "\"    chunk = records[i:i+batch_size]\\n\"\n",
    "\"    values_sql = []\\n\"\n",
    "\"    params = {}\\n\"\n",
    "\"    for j, rec in enumerate(chunk):\\n\"\n",
    "\"        key = f'rec_{i+j}'\\n\"\n",
    "\"        params[key] = json.dumps(rec, ensure_ascii=False)\\n\" \n",
    "\"        params[f'src_{i+j}'] = str(Path(RAW_FILE))\\n\"\n",
    "\"        values_sql.append(f'(:src_{i+j}, CAST(:{key} AS JSONB))')\\n\"\n",
    "\"    sql = 'INSERT INTO bronze.raw_ingest (source_file, payload_json) VALUES ' + ','.join(values_sql) + ';'\\n\"\n",
    "\"    with app_engine.begin() as conn:\\n\"\n",
    "\"        conn.execute(text(sql), params)\\n\"\n",
    "\"    total += len(chunk)\\n\"\n",
    "\"print('✅ Insertions effectuées:', total)\\n\"\n",
    "))\n",
    "\n",
    "cells.append(nbf.v4.new_markdown_cell(\"## 5) Contrôles rapides\"))\n",
    "cells.append(nbf.v4.new_code_cell(\n",
    "\"import pandas as pd\\n\"\n",
    "\"from sqlalchemy import text\\n\\n\"\n",
    "\"with app_engine.connect() as conn:\\n\"\n",
    "\"    n = conn.execute(text('SELECT COUNT(*) FROM bronze.raw_ingest')).scalar_one()\\n\"\n",
    "\"    print('📊 Total lignes en bronze.raw_ingest:', n)\\n\"\n",
    "\"    sample = pd.read_sql(text(\\n\"\n",
    "\"        'SELECT id_big, source_file, left(payload_json::text, 200) AS payload_preview, ingested_at '\\n\"\n",
    "\"        'FROM bronze.raw_ingest ORDER BY id_big DESC LIMIT 5'\\n\"\n",
    "\"    ), conn)\\n\"\n",
    "\"\\n\"\n",
    "\"sample\\n\"\n",
    "))\n",
    "\n",
    "nb['cells'] = cells\n",
    "\n",
    "out_path = Path('/mnt/data/bronze_ingest_accidents.ipynb')\n",
    "out_path.write_bytes(nbf.writes(nb).encode('utf-8'))\n",
    "out_path.as_posix()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db1583a",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Connexion PostgreSQL (helpers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beef24cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.engine import Engine\n",
    "\n",
    "def mk_url(user, pwd, db):\n",
    "    return f\"postgresql+psycopg2://{user}:{pwd}@{PG_HOST}:{PG_PORT}/{db}\"\n",
    "\n",
    "def get_engine(user, pwd, db) -> Engine:\n",
    "    return create_engine(mk_url(user, pwd, db), pool_pre_ping=True, future=True)\n",
    "\n",
    "def run_sql(engine: Engine, sql: str, params: dict|None=None):\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(sql), params or {})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b47b340",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Bootstrap BDD (rôles, base, schémas, droits)\n",
    "\n",
    "Cette cellule est **idempotente** : elle peut être relancée sans casser l'état.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cc44ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# On se connecte d'abord au superuser sur la base 'postgres' pour créer user & base\n",
    "su_engine = get_engine(PG_SU_USER, PG_SU_PASS, 'postgres')\n",
    "\n",
    "# 3.1 Créer le rôle applicatif (mot de passe commun en local)\n",
    "run_sql(su_engine, f\"\"\"\n",
    "DO $$\n",
    "BEGIN\n",
    "   IF NOT EXISTS (SELECT FROM pg_catalog.pg_roles WHERE rolname = '{PG_USER}') THEN\n",
    "      CREATE ROLE {PG_USER} LOGIN PASSWORD '{PG_PASS}';\n",
    "   END IF;\n",
    "END$$;\n",
    "\"\"\")\n",
    "\n",
    "# 3.2 Créer la base si absente + owner\n",
    "run_sql(su_engine, f\"\"\"\n",
    "DO $$\n",
    "BEGIN\n",
    "   IF NOT EXISTS (SELECT FROM pg_database WHERE datname = '{PG_DB}') THEN\n",
    "      CREATE DATABASE {PG_DB} OWNER {PG_USER};\n",
    "   END IF;\n",
    "END$$;\n",
    "\"\"\")\n",
    "\n",
    "# 3.3 Schémas & droits par défaut\n",
    "app_engine = get_engine(PG_USER, PG_PASS, PG_DB)\n",
    "\n",
    "run_sql(app_engine, \"\"\"\n",
    "CREATE SCHEMA IF NOT EXISTS bronze AUTHORIZATION current_user;\n",
    "CREATE SCHEMA IF NOT EXISTS silver AUTHORIZATION current_user;\n",
    "CREATE SCHEMA IF NOT EXISTS gold   AUTHORIZATION current_user;\n",
    "\"\"\")\n",
    "\n",
    "# Droits par défaut (lecture/écriture bronze/silver, lecture gold)\n",
    "run_sql(app_engine, \"\"\"\n",
    "ALTER DEFAULT PRIVILEGES IN SCHEMA bronze GRANT SELECT, INSERT, UPDATE, DELETE ON TABLES TO current_user;\n",
    "ALTER DEFAULT PRIVILEGES IN SCHEMA silver GRANT SELECT, INSERT, UPDATE, DELETE ON TABLES TO current_user;\n",
    "ALTER DEFAULT PRIVILEGES IN SCHEMA gold   GRANT SELECT ON TABLES TO current_user;\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Bootstrap OK — base, rôle, schémas prêts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b4d742",
   "metadata": {},
   "source": [
    "\n",
    "## 4) DDL BRONZE (tables + index)\n",
    "> **Adapte** la structure à votre dictionnaire. Cette version stocke les champs clés + le brut JSON.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab57f5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bronze_sql = '''\n",
    "CREATE TABLE IF NOT EXISTS bronze.accidents (\n",
    "  id_accident        TEXT PRIMARY KEY,\n",
    "  date_heure         TIMESTAMPTZ,\n",
    "  departement_code   TEXT,\n",
    "  commune_code       TEXT,\n",
    "  gravite            TEXT,\n",
    "  type_route         TEXT,\n",
    "  condition_meteo    TEXT,\n",
    "  luminosite         TEXT,\n",
    "  nb_vehicules       INT,\n",
    "  payload_json       JSONB\n",
    ");\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS ix_accidents_date     ON bronze.accidents(date_heure);\n",
    "CREATE INDEX IF NOT EXISTS ix_accidents_commune  ON bronze.accidents(commune_code);\n",
    "CREATE INDEX IF NOT EXISTS ix_accidents_gravite  ON bronze.accidents(gravite);\n",
    "'''\n",
    "run_sql(app_engine, bronze_sql)\n",
    "print(\"✅ DDL BRONZE créé/à jour.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdd66fb",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Ingestion API → BRONZE\n",
    "\n",
    "Source : Opendatasoft — *accidents-corporels-de-la-circulation-millesime*  \n",
    "Pagination **par pages de 1000** (modifiable). On persiste un CSV en `data/raw/` et on **upsert** en base.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6145f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests, time, json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "BASE_URL = \"https://public.opendatasoft.com/api/records/1.0/search/\"\n",
    "DATASET = \"accidents-corporels-de-la-circulation-millesime\"\n",
    "\n",
    "RAW_DIR = Path(\"data/raw\")\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def fetch_page(offset=0, limit=1000):\n",
    "    params = {\"dataset\": DATASET, \"rows\": limit, \"start\": offset}\n",
    "    r = requests.get(BASE_URL, params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json().get(\"records\", [])\n",
    "\n",
    "# 5.1 Récupération (limite paramétrable)\n",
    "page_size = 1000\n",
    "max_pages  = 5   # ← ajuste (None = jusqu’au bout, attention au temps)\n",
    "records = []\n",
    "offset = 0\n",
    "pages = 0\n",
    "\n",
    "while True:\n",
    "    page = fetch_page(offset, page_size)\n",
    "    if not page:\n",
    "        break\n",
    "    records.extend(page)\n",
    "    offset += page_size\n",
    "    pages += 1\n",
    "    if max_pages and pages >= max_pages:\n",
    "        break\n",
    "    time.sleep(0.2)\n",
    "\n",
    "print(f\"📦 Récupérés: {len(records)} en {pages} pages.\")\n",
    "\n",
    "# 5.2 Sauvegarde brute\n",
    "raw_path = RAW_DIR / \"accidents_raw.jsonl\"\n",
    "with raw_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for rec in records:\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"💾 Écrit: {raw_path.resolve()}\")\n",
    "\n",
    "# 5.3 Normalisation minimale → DataFrame\n",
    "rows = []\n",
    "for rec in records:\n",
    "    fid = rec.get(\"fields\", {})\n",
    "\n",
    "    rows.append({\n",
    "        \"id_accident\": rec.get(\"recordid\"),\n",
    "        \"date_heure\": fid.get(\"date\", None),\n",
    "        \"departement_code\": fid.get(\"departement\", None),\n",
    "        \"commune_code\": fid.get(\"com_code\", None) or fid.get(\"com\", None),\n",
    "        \"gravite\": fid.get(\"gravite\", None) or fid.get(\"grav\", None),\n",
    "        \"type_route\": fid.get(\"catr\", None) or fid.get(\"type_voie\", None),\n",
    "        \"condition_meteo\": fid.get(\"atm\", None) or fid.get(\"conditions_atmospheriques\", None),\n",
    "        \"luminosite\": fid.get(\"lum\", None) or fid.get(\"luminosite\", None),\n",
    "        \"nb_vehicules\": fid.get(\"nbv\", None),\n",
    "        \"payload_json\": rec.get(\"fields\", {}),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df['date_heure'] = pd.to_datetime(df['date_heure'], errors='coerce', utc=True)\n",
    "print(df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505f9fef",
   "metadata": {},
   "source": [
    "\n",
    "### 5.4 Chargement BRONZE (UPSERT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sqlalchemy.dialects.postgresql import insert\n",
    "\n",
    "with app_engine.begin() as conn:\n",
    "    tbl_cols = [\"id_accident\",\"date_heure\",\"departement_code\",\"commune_code\",\n",
    "                \"gravite\",\"type_route\",\"condition_meteo\",\"luminosite\",\n",
    "                \"nb_vehicules\",\"payload_json\"]\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        stmt = insert(text(\"bronze.accidents\")).values({c: row[c] for c in tbl_cols})\n",
    "        # SQLAlchemy core + text doesn't support insert(...).on_conflict_do_update with plain text table easily,\n",
    "        # so we fallback to a MERGE-like pattern using raw SQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc85350",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def upsert_bronze_batch(engine, df, batch_size=1000):\n",
    "    # Simple batched UPSERT using native SQL to keep it engine-agnostic\n",
    "    import math\n",
    "    cols = [\"id_accident\",\"date_heure\",\"departement_code\",\"commune_code\",\n",
    "            \"gravite\",\"type_route\",\"condition_meteo\",\"luminosite\",\n",
    "            \"nb_vehicules\",\"payload_json\"]\n",
    "    total = len(df)\n",
    "    for i in tqdm(range(0, total, batch_size)):\n",
    "        chunk = df.iloc[i:i+batch_size].copy()\n",
    "        # Build VALUES list\n",
    "        values_sql = []\n",
    "        params = {}\n",
    "        for j, (_, r) in enumerate(chunk.iterrows()):\n",
    "            placeholders = []\n",
    "            for c in cols:\n",
    "                key = f\"{c}_{i+j}\"\n",
    "                val = r[c]\n",
    "                params[key] = val\n",
    "                placeholders.append(f\":{key}\")\n",
    "            values_sql.append(\"(\" + \",\".join(placeholders) + \")\")\n",
    "        sql = f'''\n",
    "        INSERT INTO bronze.accidents ({\",\".join(cols)})\n",
    "        VALUES {\",\".join(values_sql)}\n",
    "        ON CONFLICT (id_accident) DO UPDATE SET\n",
    "            date_heure = EXCLUDED.date_heure,\n",
    "            departement_code = EXCLUDED.departement_code,\n",
    "            commune_code = EXCLUDED.commune_code,\n",
    "            gravite = EXCLUDED.gravite,\n",
    "            type_route = EXCLUDED.type_route,\n",
    "            condition_meteo = EXCLUDED.condition_meteo,\n",
    "            luminosite = EXCLUDED.luminosite,\n",
    "            nb_vehicules = EXCLUDED.nb_vehicules,\n",
    "            payload_json = EXCLUDED.payload_json;\n",
    "        '''\n",
    "        with app_engine.begin() as conn:\n",
    "            conn.execute(text(sql), params)\n",
    "\n",
    "upsert_bronze_batch(app_engine, df, batch_size=1000)\n",
    "print(\"✅ BRONZE chargé (UPSERT).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db36b72b",
   "metadata": {},
   "source": [
    "\n",
    "## 6) SILVER — vues nettoyées/typées (exemple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dbeb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "silver_sql = '''\n",
    "CREATE OR REPLACE VIEW silver.accidents_clean AS\n",
    "SELECT\n",
    "  id_accident,\n",
    "  date_heure,\n",
    "  COALESCE(departement_code, payload_json->>'dep')      AS departement_code,\n",
    "  COALESCE(commune_code,     payload_json->>'com')      AS commune_code,\n",
    "  gravite,\n",
    "  type_route,\n",
    "  condition_meteo,\n",
    "  luminosite,\n",
    "  nb_vehicules\n",
    "FROM bronze.accidents;\n",
    "'''\n",
    "run_sql(app_engine, silver_sql)\n",
    "print(\"✅ Vues SILVER prêtes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad3f6f2",
   "metadata": {},
   "source": [
    "\n",
    "## 7) GOLD — modèle dimensionnel minimal (exemple)\n",
    "- **dim_temps**, **dim_lieu**, **dim_conditions**, **fact_accidents**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d179d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gold_sql = '''\n",
    "CREATE TABLE IF NOT EXISTS gold.dim_temps (\n",
    "  id_temps SERIAL PRIMARY KEY,\n",
    "  date_heure TIMESTAMPTZ UNIQUE,\n",
    "  annee INT, mois INT, semaine INT, jour INT, heure INT\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS gold.dim_lieu (\n",
    "  id_lieu SERIAL PRIMARY KEY,\n",
    "  departement_code TEXT,\n",
    "  commune_code TEXT,\n",
    "  UNIQUE(departement_code, commune_code)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS gold.dim_conditions (\n",
    "  id_cond SERIAL PRIMARY KEY,\n",
    "  gravite TEXT,\n",
    "  type_route TEXT,\n",
    "  condition_meteo TEXT,\n",
    "  luminosite TEXT,\n",
    "  UNIQUE(gravite, type_route, condition_meteo, luminosite)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS gold.fact_accidents (\n",
    "  id_accident TEXT PRIMARY KEY,\n",
    "  id_temps INT REFERENCES gold.dim_temps(id_temps),\n",
    "  id_lieu  INT REFERENCES gold.dim_lieu(id_lieu),\n",
    "  id_cond  INT REFERENCES gold.dim_conditions(id_cond),\n",
    "  nb_vehicules INT\n",
    ");\n",
    "'''\n",
    "run_sql(app_engine, gold_sql)\n",
    "print(\"✅ Tables GOLD créées.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0630c9",
   "metadata": {},
   "source": [
    "\n",
    "### 7.1 Peuplement GOLD (ETL simple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0096c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Temps\n",
    "run_sql(app_engine, '''\n",
    "INSERT INTO gold.dim_temps (date_heure, annee, mois, semaine, jour, heure)\n",
    "SELECT DISTINCT date_heure,\n",
    "       EXTRACT(YEAR  FROM date_heure)::INT,\n",
    "       EXTRACT(MONTH FROM date_heure)::INT,\n",
    "       EXTRACT(WEEK  FROM date_heure)::INT,\n",
    "       EXTRACT(DAY   FROM date_heure)::INT,\n",
    "       EXTRACT(HOUR  FROM date_heure)::INT\n",
    "FROM silver.accidents_clean\n",
    "WHERE date_heure IS NOT NULL\n",
    "ON CONFLICT (date_heure) DO NOTHING;\n",
    "''')\n",
    "\n",
    "# Lieu\n",
    "run_sql(app_engine, '''\n",
    "INSERT INTO gold.dim_lieu (departement_code, commune_code)\n",
    "SELECT DISTINCT departement_code, commune_code\n",
    "FROM silver.accidents_clean\n",
    "ON CONFLICT (departement_code, commune_code) DO NOTHING;\n",
    "''')\n",
    "\n",
    "# Conditions\n",
    "run_sql(app_engine, '''\n",
    "INSERT INTO gold.dim_conditions (gravite, type_route, condition_meteo, luminosite)\n",
    "SELECT DISTINCT gravite, type_route, condition_meteo, luminosite\n",
    "FROM silver.accidents_clean\n",
    "ON CONFLICT (gravite, type_route, condition_meteo, luminosite) DO NOTHING;\n",
    "''')\n",
    "\n",
    "# Fait\n",
    "run_sql(app_engine, '''\n",
    "INSERT INTO gold.fact_accidents (id_accident, id_temps, id_lieu, id_cond, nb_vehicules)\n",
    "SELECT s.id_accident,\n",
    "       t.id_temps,\n",
    "       l.id_lieu,\n",
    "       c.id_cond,\n",
    "       s.nb_vehicules\n",
    "FROM silver.accidents_clean s\n",
    "JOIN gold.dim_temps t ON t.date_heure = s.date_heure\n",
    "JOIN gold.dim_lieu  l ON l.departement_code = s.departement_code AND l.commune_code = s.commune_code\n",
    "JOIN gold.dim_conditions c ON c.gravite = s.gravite AND c.type_route = s.type_route\n",
    "                           AND c.condition_meteo = s.condition_meteo AND c.luminosite = s.luminosite\n",
    "ON CONFLICT (id_accident) DO NOTHING;\n",
    "''')\n",
    "\n",
    "print(\"✅ GOLD peuplé.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cd9072",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Analyses (exemples demandés)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97528fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def query_df(sql):\n",
    "    with app_engine.connect() as conn:\n",
    "        return pd.read_sql(text(sql), conn)\n",
    "\n",
    "# 8.1 Conditions risquées vs moyenne nationale\n",
    "q1 = '''\n",
    "WITH national AS (\n",
    "  SELECT AVG(nb_vehicules) AS avg_nb FROM gold.fact_accidents\n",
    ")\n",
    "SELECT c.gravite, c.type_route, c.condition_meteo, c.luminosite,\n",
    "       COUNT(*) AS accidents, AVG(f.nb_vehicules) AS avg_nb\n",
    "FROM gold.fact_accidents f\n",
    "JOIN gold.dim_conditions c ON f.id_cond = c.id_cond\n",
    "GROUP BY 1,2,3,4\n",
    "ORDER BY accidents DESC\n",
    "LIMIT 20;\n",
    "'''\n",
    "df1 = query_df(q1); df1.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66679ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8.2 Zones fréquentées : plus d'accidents graves ou juste plus d'accidents ?\n",
    "q2 = '''\n",
    "SELECT l.departement_code, l.commune_code,\n",
    "       COUNT(*) AS total_accidents,\n",
    "       SUM(CASE WHEN c.gravite IN ('3','4','grave','mortel') THEN 1 ELSE 0 END) AS accidents_graves\n",
    "FROM gold.fact_accidents f\n",
    "JOIN gold.dim_lieu l ON f.id_lieu = l.id_lieu\n",
    "JOIN gold.dim_conditions c ON f.id_cond = c.id_cond\n",
    "GROUP BY 1,2\n",
    "ORDER BY total_accidents DESC\n",
    "LIMIT 20;\n",
    "'''\n",
    "df2 = query_df(q2); df2.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a2cfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8.3 Détection de semaines anormales (écart à la moyenne)\n",
    "q3 = '''\n",
    "WITH weekly AS (\n",
    "  SELECT t.annee, t.semaine, COUNT(*) AS nb\n",
    "  FROM gold.fact_accidents f\n",
    "  JOIN gold.dim_temps t ON f.id_temps = t.id_temps\n",
    "  GROUP BY t.annee, t.semaine\n",
    "),\n",
    "stats AS (\n",
    "  SELECT AVG(nb) AS mu, STDDEV_POP(nb) AS sigma FROM weekly\n",
    ")\n",
    "SELECT w.*, ROUND((w.nb - s.mu) / NULLIF(s.sigma,0), 2) AS zscore\n",
    "FROM weekly w CROSS JOIN stats s\n",
    "ORDER BY zscore DESC NULLS LAST\n",
    "LIMIT 20;\n",
    "'''\n",
    "df3 = query_df(q3); df3.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff2c2d3",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Indexation & partitionnement (pistes)\n",
    "- Index typiques : `fact_accidents(id_temps)`, `fact_accidents(id_lieu)`, `dim_temps(date_heure)`.\n",
    "- Partitionnement par **année** sur `dim_temps` ou directement une table de faits partitionnée par `date_heure` (via clé étrangère).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f958cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_sql(app_engine, '''\n",
    "CREATE INDEX IF NOT EXISTS ix_fact_time ON gold.fact_accidents(id_temps);\n",
    "CREATE INDEX IF NOT EXISTS ix_fact_lieu ON gold.fact_accidents(id_lieu);\n",
    "CREATE INDEX IF NOT EXISTS ix_dim_temps_date ON gold.dim_temps(date_heure);\n",
    "''')\n",
    "print(\"✅ Index conseillés créés (si absents).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7947c721",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Résumé\n",
    "- **Tout** est exécutable ici (sans Docker).\n",
    "- Idempotent : tu peux relancer les cellules sans casser la base.\n",
    "- Adapte les DDL **BRONZE/SILVER/GOLD** à votre dictionnaire réel.\n",
    "- Tu peux ouvrir DBeaver si tu veux une vue graphique, mais **tout passe par ce notebook**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
