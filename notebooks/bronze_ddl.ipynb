{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9140756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy import text\n",
    "from psycopg2.extras import execute_values\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "CLEAN_DIR = Path(\"../data/cleaned\")\n",
    "RAW_DIR = Path(\"../data/raw\")\n",
    "CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d270e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADMIN_DB_URL = \"postgresql+psycopg2://postgres:postgres2025%40@localhost:54785/postgres\"\n",
    "DB_NAME = \"db_accident\"\n",
    "DB_URL = ADMIN_DB_URL.rsplit(\"/\", 1)[0] + f\"/{DB_NAME}\" \n",
    "\n",
    "\n",
    "def get_engine(db_url: str) -> Engine:\n",
    "    \"\"\"\n",
    "    Retourne un SQLAlchemy Engine pour l'URL \n",
    "    postgresql+psycopg2://user:password@host:port/dbname\n",
    "    \"\"\"\n",
    "    return create_engine(db_url, client_encoding=\"utf8\")\n",
    "\n",
    "\n",
    "def show_open_connections(engine, db_name: str = None):\n",
    "    \"\"\"\n",
    "    Affiche les connexions ouvertes à une base PostgreSQL.\n",
    "    Si db_name est fourni, on filtre sur cette base.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT pid, usename, datname, state, query\n",
    "    FROM pg_stat_activity\n",
    "    WHERE datname = :db_name;\n",
    "    \"\"\" if db_name else \"\"\"\n",
    "    SELECT pid, usename, datname, state, query\n",
    "    FROM pg_stat_activity;\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(query), {\"db_name\": db_name})\n",
    "        rows = result.fetchall()\n",
    "        for r in rows:\n",
    "            print(r)\n",
    "\n",
    "\n",
    "def close_all_connections(engine, db_name: str):\n",
    "    \"\"\"\n",
    "    Ferme toutes les connexions PostgreSQL actives sur une base donnée (y compris celle en cours).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query = text(\"\"\"\n",
    "            SELECT pg_terminate_backend(pid)\n",
    "            FROM pg_stat_activity\n",
    "            WHERE datname = :db_name\n",
    "            AND pid <> pg_backend_pid();\n",
    "        \"\"\")\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(query, {\"db_name\": db_name})\n",
    "        logger.info(\"Toutes les connexions à '%s' ont été fermées (sauf la session actuelle).\", db_name)\n",
    "    except Exception as e:\n",
    "        logger.error(\" Erreur lors de la fermeture des connexions : %s\", e)\n",
    "        raise\n",
    "    finally:\n",
    "        # Ferme la dernière connexion (celle utilisée par SQLAlchemy)\n",
    "        engine.dispose()\n",
    "        logger.info(\"Engine SQLAlchemy libéré — plus aucune connexion active.\")\n",
    "\n",
    "\n",
    "def create_database_if_not_exists(admin_db_url: str, db_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Crée une base PostgreSQL si elle n'existe pas.\n",
    "    \"\"\"\n",
    "    db_url = admin_db_url.rsplit(\"/\", 1)[0] + f\"/{db_name}\"  # Construire l'URL cible\n",
    "\n",
    "    try:\n",
    "        if not database_exists(db_url):\n",
    "            create_database(db_url)  # Appel correct à sqlalchemy_utils\n",
    "            logger.info(\"Base '%s' créée avec succès\", db_name)\n",
    "        else:\n",
    "            logger.info(\"La base '%s' existe déjà\", db_name)\n",
    "    except Exception as e:\n",
    "        logger.error(\"Erreur lors de la création de la base '%s' : %s\", db_name, e)\n",
    "        raise\n",
    "    \n",
    "\n",
    "def execute_script(engine: Engine, script_sql: str) -> None:\n",
    "    \"\"\"\n",
    "    Exécute un script SQL donné sur la base de données.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Exécution du script SQL...\")\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(script_sql))\n",
    "        logger.info(\"Script SQL exécuté avec succès\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"Erreur lors de l'exécution du script SQL : %s\", e)\n",
    "        raise\n",
    "    finally:\n",
    "        engine.dispose()\n",
    "        logger.info(\"Connexion fermée et engine libéré.\")\n",
    "\n",
    "\n",
    "def insert_df_to_table(\n",
    "    engine: Engine,\n",
    "    df: pd.DataFrame,\n",
    "    schema: str,\n",
    "    table: str,\n",
    "    table_columns: Optional[List[str]] = None,\n",
    "    batch_size: int = 1000,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Insère un DataFrame dans une table Postgres via psycopg2.execute_values (performant pour gros volumes).\n",
    "\n",
    "    Comportement :\n",
    "      - Si table_columns est fourni : on utilise cet ordre de colonnes pour l'insertion.\n",
    "        * Les colonnes absentes dans df sont créées et remplies par NULL.\n",
    "        * Les colonnes supplémentaires dans df sont ignorées.\n",
    "      - Si table_columns est None : on utilise l'ordre des colonnes présentes dans df.\n",
    "      - Retourne le nombre de lignes insérées.\n",
    "      - Gère commit/rollback automatiquement.\n",
    "\n",
    "    Arguments :\n",
    "      engine        : SQLAlchemy Engine (obtenu via get_engine)\n",
    "      df            : pandas.DataFrame (les colonnes doivent déjà être normalisées)\n",
    "      schema        : schéma SQL (ex. \"bronze\")\n",
    "      table         : nom de la table (ex. \"caracteristiques_raw\")\n",
    "      table_columns : liste ordonnée des colonnes à insérer (optionnel)\n",
    "      batch_size    : taille de page pour execute_values (par défaut 1000)\n",
    "\n",
    "    Remarques :\n",
    "      - Nécessite psycopg2 (sqlalchemy devra utiliser l'adaptateur psycopg2).\n",
    "      - Tous types Python usuels sont supportés (str, int, float, None, bool, datetime, ...).\n",
    "    \"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        return 0\n",
    "\n",
    "    # Déterminer colonnes cibles et préparer df_subset avec la bonne colonne ordre\n",
    "    if table_columns is None:\n",
    "        cols = list(df.columns)\n",
    "    else:\n",
    "        cols = list(table_columns)\n",
    "        # ajouter colonnes manquantes dans df en les remplissant avec None\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                df[c] = None\n",
    "\n",
    "    # Conserver uniquement les colonnes cibles\n",
    "    df_subset = df[cols].copy()\n",
    "\n",
    "    # Convertir les NaN pandas en None pour psycopg2\n",
    "    df_subset = df_subset.where(pd.notnull(df_subset), None)\n",
    "\n",
    "    # Préparer les tuples de valeurs\n",
    "    records = [tuple(x) for x in df_subset.itertuples(index=False, name=None)]\n",
    "    if not records:\n",
    "        return 0\n",
    "\n",
    "    # Construire la clause des colonnes (avec guillemets pour noms contenant underscore ou majuscules)\n",
    "    cols_sql = \", \".join([f'\"{c}\"' for c in cols])\n",
    "    insert_sql = f'INSERT INTO \"{schema}\".\"{table}\" ({cols_sql}) VALUES %s'\n",
    "\n",
    "    # Obtenir une connexion psycopg2 à partir de l'engine SQLAlchemy\n",
    "    conn = engine.raw_connection()\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        # execute_values fait des inserts par batch très efficaces\n",
    "        execute_values(cur, insert_sql, records, page_size=batch_size)\n",
    "        conn.commit()\n",
    "        return len(records)\n",
    "    except Exception:\n",
    "        conn.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "def close_engine(engine: Engine) -> None:\n",
    "    \"\"\"\n",
    "    Ferme proprement toutes les connexions actives et libère les ressources associées à l'Engine SQLAlchemy.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fermer les connexions éventuelles encore ouvertes\n",
    "        if hasattr(engine, \"dispose\"):\n",
    "            engine.dispose()\n",
    "            logger.info(\"Connexion et engine SQLAlchemy correctement fermés.\")\n",
    "        else:\n",
    "            logger.warning(\"L'objet fourni ne semble pas être un Engine SQLAlchemy valide.\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"Erreur lors de la fermeture de l'engine : %s\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2c8fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# DDL : bronze \n",
    "# ---------------------------\n",
    "script_bronze_dll = \"\"\"\n",
    "CREATE SCHEMA IF NOT EXISTS bronze;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS bronze.caracteristiques_raw (\n",
    "  identifiant_de_l_accident TEXT,\n",
    "  date_et_heure TEXT,\n",
    "  commune TEXT,\n",
    "  annee TEXT,\n",
    "  mois TEXT,\n",
    "  jour TEXT,\n",
    "  heure_minute TEXT,\n",
    "  lumiere TEXT,\n",
    "  localisation TEXT,\n",
    "  intersection TEXT,\n",
    "  conditions_atmospheriques TEXT,\n",
    "  collision TEXT,\n",
    "  departement TEXT,\n",
    "  code_commune TEXT,\n",
    "  code_insee TEXT,\n",
    "  adresse TEXT,\n",
    "  latitude TEXT,\n",
    "  longitude TEXT,\n",
    "  code_postal TEXT,\n",
    "  numero TEXT,\n",
    "  coordonnees TEXT,\n",
    "  pr TEXT,\n",
    "  surface TEXT,\n",
    "  v1 TEXT,\n",
    "  circulation TEXT,\n",
    "  voie_reservee TEXT,\n",
    "  env1 TEXT,\n",
    "  voie TEXT,\n",
    "  largeur_de_la_chaussee TEXT,\n",
    "  v2 TEXT,\n",
    "  largeur_terre_plein_central TEXT,\n",
    "  nombre_de_voies TEXT,\n",
    "  categorie_route TEXT,\n",
    "  pr1 TEXT,\n",
    "  plan TEXT,\n",
    "  profil TEXT,\n",
    "  infrastructure TEXT,\n",
    "  situation TEXT,\n",
    "  gps TEXT,\n",
    "  date TEXT,\n",
    "  year_georef TEXT,\n",
    "  nom_officiel_commune TEXT,\n",
    "  code_officiel_departement TEXT,\n",
    "  nom_officiel_departement TEXT,\n",
    "  code_officiel_epci TEXT,\n",
    "  nom_officiel_epci TEXT,\n",
    "  code_officiel_region TEXT,\n",
    "  nom_officiel_region TEXT,\n",
    "  nom_officiel_commune_arrondissement_municipal TEXT,\n",
    "  code_officiel_commune TEXT,\n",
    "  source_file TEXT,\n",
    "  ingest_ts TIMESTAMP WITH TIME ZONE DEFAULT now()\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS bronze.lieux_raw (\n",
    "  identifiant_de_l_accident TEXT,\n",
    "  categorie_route TEXT,\n",
    "  voie TEXT,\n",
    "  v1 TEXT,\n",
    "  v2 TEXT,\n",
    "  circulation TEXT,\n",
    "  nombre_de_voies TEXT,\n",
    "  voie_reservee TEXT,\n",
    "  profil TEXT,\n",
    "  pr TEXT,\n",
    "  pr1 TEXT,\n",
    "  plan TEXT,\n",
    "  largeur_terre_plein_central TEXT,\n",
    "  largeur_de_la_chaussee TEXT,\n",
    "  surface TEXT,\n",
    "  infrastructure TEXT,\n",
    "  situation TEXT,\n",
    "  env1 TEXT,\n",
    "  coordonnees TEXT,\n",
    "  source_file TEXT,\n",
    "  ingest_ts TIMESTAMP WITH TIME ZONE DEFAULT now()\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS bronze.vehicules_raw (\n",
    "  identifiant_de_l_accident TEXT,\n",
    "  identifiant_vehicule TEXT,\n",
    "  sens TEXT,\n",
    "  categorie_vehicule TEXT,\n",
    "  obstacle_fixe_heurte TEXT,\n",
    "  obstacle_mobile_heurte TEXT,\n",
    "  point_de_choc TEXT,\n",
    "  manoeuvre TEXT,\n",
    "  nombre_d_occupants TEXT,\n",
    "  source_file TEXT,\n",
    "  ingest_ts TIMESTAMP WITH TIME ZONE DEFAULT now()\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS bronze.usagers_raw (\n",
    "  identifiant_de_l_accident TEXT,\n",
    "  identifiant_vehicule TEXT,\n",
    "  place TEXT,\n",
    "  categorie_d_usager TEXT,\n",
    "  gravite TEXT,\n",
    "  sexe TEXT,\n",
    "  annee_de_naissance TEXT,\n",
    "  motif_trajet TEXT,\n",
    "  existence_equipement_de_securite TEXT,\n",
    "  utilisation_equipement_de_securite TEXT,\n",
    "  localisation_du_pieton TEXT,\n",
    "  action_pieton TEXT,\n",
    "  pieton_seul_ou_non TEXT,\n",
    "  source_file TEXT,\n",
    "  ingest_ts TIMESTAMP WITH TIME ZONE DEFAULT now()\n",
    ");\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS br_car_ident_idx ON bronze.caracteristiques_raw (identifiant_de_l_accident);\n",
    "CREATE INDEX IF NOT EXISTS br_lieux_ident_idx ON bronze.lieux_raw (identifiant_de_l_accident);\n",
    "CREATE INDEX IF NOT EXISTS br_veh_ident_idx ON bronze.vehicules_raw (identifiant_de_l_accident);\n",
    "CREATE INDEX IF NOT EXISTS br_usg_ident_idx ON bronze.usagers_raw (identifiant_de_l_accident);\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS br_car_dep_idx ON bronze.caracteristiques_raw (departement);\n",
    "CREATE INDEX IF NOT EXISTS br_car_date_idx ON bronze.caracteristiques_raw (annee, mois);\n",
    "CREATE INDEX IF NOT EXISTS br_veh_catv_idx ON bronze.vehicules_raw (categorie_vehicule);\n",
    "CREATE INDEX IF NOT EXISTS br_usg_grav_idx ON bronze.usagers_raw (gravite);\n",
    "\"\"\"\n",
    "create_database_if_not_exists(ADMIN_DB_URL, DB_NAME)\n",
    "engine = get_engine(DB_URL)\n",
    "execute_script(engine, script_bronze_dll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e25499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_name = \"accidents_clean.csv\"\n",
    "# try:\n",
    "#     df_cleaned = pd.read_csv(CLEAN_DIR / csv_name,\n",
    "#                      sep=\";\", dtype=str, encoding=\"utf-8-sig\")\n",
    "# except UnicodeDecodeError:\n",
    "#     df_cleaned = pd.read_csv(CLEAN_DIR / csv_name,\n",
    "#                      sep=\";\", dtype=str, encoding=\"utf-8-sig\")\n",
    "\n",
    "# print(\"Colonnes détectées:\", list(df.columns)[:8], \"...\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ce travail dois etre fait apres l'importation des données brutes et stockéés\n",
    "# et generer les resultats sous forme de fichier accidents_clean.csv \n",
    "# comme ça on utilise directement le code commenterci dessus\n",
    "csv_name = \"accidents-corporels-de-la-circulation-millesime.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(RAW_DIR / csv_name,\n",
    "                     sep=\";\", dtype=str, encoding=\"utf-8-sig\")\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(RAW_DIR / csv_name,\n",
    "                     sep=\";\", dtype=str, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Colonnes détectées:\", list(df.columns)[:8], \"...\")\n",
    "\n",
    "\n",
    "def normalize_columns(df, inplace=False):\n",
    "    \"\"\"\n",
    "    Normalise les noms de colonnes d'un DataFrame en snake_case sans accents ni caractères spéciaux.\n",
    "    Règles :\n",
    "      - retire les accents\n",
    "      - met en minuscules\n",
    "      - remplace tout caractère non alphanumérique par un underscore\n",
    "      - réduit les underscores multiples en un seul\n",
    "      - supprime les underscores en début/fin\n",
    "      - si le nom commence par un chiffre, préfixe par 'c_'\n",
    "      - si le résultat est vide, remplace par 'unknown'\n",
    "      - garantit l'unicité des noms en ajoutant des suffixes _2, _3, ...\n",
    "    Arguments :\n",
    "      df : pandas.DataFrame\n",
    "      inplace : bool (False par défaut). Si True, renomme les colonnes sur place et retourne le même objet.\n",
    "    Retour :\n",
    "      pandas.DataFrame avec colonnes normalisées.\n",
    "    \"\"\"\n",
    "    import unicodedata\n",
    "    import re\n",
    "    import pandas as pd\n",
    "\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "\n",
    "    def _slug(name: object) -> str:\n",
    "        s = \"\" if name is None else str(name)\n",
    "        # Normaliser unicode et séparer les accents\n",
    "        s = unicodedata.normalize(\"NFKD\", s)\n",
    "        # Enlever les caractères de composition (accents)\n",
    "        s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "        s = s.lower()\n",
    "        # Remplacer tout caractère non alphanumérique par underscore\n",
    "        s = re.sub(r\"[^a-z0-9]+\", \"_\", s)\n",
    "        # Réduire underscores multiples et trim\n",
    "        s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "        # Préfixer si commence par chiffre\n",
    "        if re.match(r\"^[0-9]\", s):\n",
    "            s = \"c_\" + s\n",
    "        if s == \"\":\n",
    "            s = \"unknown\"\n",
    "        return s\n",
    "\n",
    "    # Appliquer la normalisation\n",
    "    orig_cols = list(df.columns)\n",
    "    normalized = [_slug(c) for c in orig_cols]\n",
    "\n",
    "    # Garantir l'unicité des noms\n",
    "    seen = {}\n",
    "    unique_cols = []\n",
    "    for name in normalized:\n",
    "        base = name\n",
    "        if name not in seen:\n",
    "            seen[name] = 1\n",
    "            unique_cols.append(name)\n",
    "        else:\n",
    "            seen[name] += 1\n",
    "            new_name = f\"{base}_{seen[name]}\"\n",
    "            # garantir que new_name lui-même n'existe pas déjà\n",
    "            while new_name in seen:\n",
    "                seen[base] += 1\n",
    "                new_name = f\"{base}_{seen[base]}\"\n",
    "            seen[new_name] = 1\n",
    "            unique_cols.append(new_name)\n",
    "\n",
    "    # Renommer le DataFrame\n",
    "    mapping = dict(zip(orig_cols, unique_cols))\n",
    "    df = df.rename(columns=mapping)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808a699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = normalize_columns(df, inplace=True)\n",
    "df_cleaned.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea14eda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_colonnes = [\n",
    "    \"identifiant_de_l_accident\",\n",
    "    \"date_et_heure\",\n",
    "    \"commune\",\n",
    "    \"annee\",\n",
    "    \"mois\",\n",
    "    \"jour\",\n",
    "    \"heure_minute\",\n",
    "    \"lumiere\",\n",
    "    \"localisation\",\n",
    "    \"intersection\",\n",
    "    \"conditions_atmospheriques\",\n",
    "    \"collision\",\n",
    "    \"departement\",\n",
    "    \"code_commune\",\n",
    "    \"code_insee\",\n",
    "    \"adresse\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"code_postal\",\n",
    "    \"numero\",\n",
    "    \"coordonnees\",\n",
    "    \"pr\",\n",
    "    \"surface\",\n",
    "    \"v1\",\n",
    "    \"circulation\",\n",
    "    \"voie_reservee\",\n",
    "    \"env1\",\n",
    "    \"voie\",\n",
    "    \"largeur_de_la_chaussee\",\n",
    "    \"v2\",\n",
    "    \"largeur_terre_plein_central\",\n",
    "    \"nombre_de_voies\",\n",
    "    \"categorie_route\",\n",
    "    \"pr1\",\n",
    "    \"plan\",\n",
    "    \"profil\",\n",
    "    \"infrastructure\",\n",
    "    \"situation\",\n",
    "    \"gps\",\n",
    "    \"date\",\n",
    "    \"year_georef\",\n",
    "    \"nom_officiel_commune\",\n",
    "    \"code_officiel_departement\",\n",
    "    \"nom_officiel_departement\",\n",
    "    \"code_officiel_epci\",\n",
    "    \"nom_officiel_epci\",\n",
    "    \"code_officiel_region\",\n",
    "    \"nom_officiel_region\",\n",
    "    \"nom_officiel_commune_arrondissement_municipal\",\n",
    "    \"code_officiel_commune\"\n",
    "]\n",
    "df_filtred_to_insert = df_cleaned[table_colonnes].copy()\n",
    "insert_df_to_table(engine, df_filtred_to_insert, \"bronze\", \"caracteristiques_raw\",\n",
    "                   table_columns=table_colonnes, batch_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a047ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_colonnes = [\n",
    "    \"identifiant_de_l_accident\",\n",
    "    \"categorie_route\",\n",
    "    \"voie\",\n",
    "    \"v1\",\n",
    "    \"v2\",\n",
    "    \"circulation\",\n",
    "    \"nombre_de_voies\",\n",
    "    \"voie_reservee\",\n",
    "    \"profil\",\n",
    "    \"pr\",\n",
    "    \"pr1\",\n",
    "    \"plan\",\n",
    "    \"largeur_terre_plein_central\",\n",
    "    \"largeur_de_la_chaussee\",\n",
    "    \"surface\",\n",
    "    \"infrastructure\",\n",
    "    \"situation\",\n",
    "    \"env1\",\n",
    "    \"coordonnees\"\n",
    "]\n",
    "df_filtred_to_insert = df_cleaned[table_colonnes].copy()\n",
    "insert_df_to_table(engine, df_filtred_to_insert, \"bronze\", \"lieux_raw\",\n",
    "                   table_columns=table_colonnes, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80da58aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changer le nom du colonne \n",
    "df_cleaned.rename(\n",
    "    columns={\n",
    "        'man_uvre': 'manoeuvre',\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "table_colonnes = [\n",
    "    \"identifiant_de_l_accident\",\n",
    "    \"identifiant_vehicule\",\n",
    "    \"sens\",\n",
    "    \"categorie_vehicule\",\n",
    "    \"obstacle_fixe_heurte\",\n",
    "    \"obstacle_mobile_heurte\",\n",
    "    \"point_de_choc\",\n",
    "    \"manoeuvre\",\n",
    "    \"nombre_d_occupants\"\n",
    "]\n",
    "\n",
    "df_filtred_to_insert = df_cleaned[table_colonnes].copy()\n",
    "insert_df_to_table(engine, df_filtred_to_insert, \"bronze\", \"vehicules_raw\",\n",
    "                   table_columns=table_colonnes, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe16515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_colonnes = [\n",
    "    \"identifiant_de_l_accident\",\n",
    "    \"identifiant_vehicule\",\n",
    "    \"place\",\n",
    "    \"categorie_d_usager\",   \n",
    "    \"gravite\",\n",
    "    \"sexe\",\n",
    "    \"annee_de_naissance\",\n",
    "    \"motif_trajet\",                         \n",
    "    \"existence_equipement_de_securite\",\n",
    "    \"utilisation_equipement_de_securite\",  \n",
    "    \"localisation_du_pieton\",            \n",
    "    \"action_pieton\",                       \n",
    "    \"pieton_seul_ou_non\"\n",
    "]\n",
    "df_filtred_to_insert = df_cleaned[table_colonnes].copy()\n",
    "insert_df_to_table(engine, df_filtred_to_insert, \"bronze\", \"usagers_raw\",\n",
    "                   table_columns=table_colonnes, batch_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee73f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_engine = get_engine(ADMIN_DB_URL)\n",
    "show_open_connections(admin_engine, DB_NAME)\n",
    "close_all_connections(admin_engine, DB_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
