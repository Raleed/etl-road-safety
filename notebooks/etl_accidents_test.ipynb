{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b1d096c",
   "metadata": {},
   "source": [
    "# ETL ‚Äî Accidents de la route\n",
    "\n",
    "Ce notebook montre, pas √† pas, comment :\n",
    "1) se connecter √† l‚ÄôAPI publique Opendatasoft,\n",
    "2) r√©cup√©rer un petit √©chantillon,\n",
    "3) paginer pour extraire un volume plus grand,\n",
    "4) sauvegarder les donn√©es brutes en CSV,\n",
    "5) poser les bases du nettoyage (√† faire en √©quipe).\n",
    "\n",
    "> **Pourquoi ce format ?**  \n",
    "> Un notebook est id√©al pour apprendre : on alterne **explications** (Markdown) et **code** (Python), et on voit les r√©sultats imm√©diatement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f38a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Dossiers de sortie\n",
    "RAW_DIR = Path(\"../data/raw\")\n",
    "CLEAN_DIR = Path(\"../data/cleaned\")\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "BASE_URL = \"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/accidents-corporels-de-la-circulation-millesime/records\"\n",
    "\n",
    "SELECT_PARTS = [\n",
    "    \"num_acc\",\n",
    "    \"datetime\",\n",
    "    \"an\",\n",
    "    \"mois\",\n",
    "    \"jour\",\n",
    "    \"hrmn\",\n",
    "    \"lum\",\n",
    "    \"agg\",\n",
    "    '\"int\" as intersection',   # <= r√©serv√© ‚Üí cit√© + alias\n",
    "    \"atm\",\n",
    "    \"col\",\n",
    "    \"dep\",\n",
    "    \"com\",\n",
    "    \"insee\",\n",
    "    \"adr\",\n",
    "    \"lat\",\n",
    "    '\"long\" as lon',           # <= r√©serv√© ‚Üí cit√© + alias\n",
    "    \"surf\",\n",
    "    \"circ\",\n",
    "    \"nbv\",\n",
    "    \"catr\",\n",
    "    \"plan\",\n",
    "    \"prof\",\n",
    "    \"infra\",\n",
    "    \"situ\",\n",
    "    \"gps\",\n",
    "    \"year_georef\",\n",
    "    \"dep_name\",\n",
    "    \"reg_name\",\n",
    "    \"epci_name\"\n",
    "]\n",
    "SELECT = \", \".join(SELECT_PARTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102745cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(offset=0, limit=1000):\n",
    "    params = {\n",
    "        \"select\": SELECT,\n",
    "        \"limit\": limit,\n",
    "        \"offset\": offset,\n",
    "        \"order_by\": \"datetime\"   # champ 'safe' pour trier\n",
    "    }\n",
    "    r = requests.get(BASE_URL, params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json().get(\"results\", [])\n",
    "\n",
    "# Test rapide\n",
    "sample = fetch_page(0, 10)\n",
    "df = pd.DataFrame(sample)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dce892",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 5000\n",
    "PAGE = 1000\n",
    "DELAY = 0.3\n",
    "\n",
    "all_rows = []\n",
    "offset = 0\n",
    "while offset < TARGET:\n",
    "    chunk = fetch_page(offset, PAGE)\n",
    "    if not chunk:\n",
    "        break\n",
    "    all_rows.extend(chunk)\n",
    "    offset += PAGE\n",
    "    time.sleep(DELAY)\n",
    "\n",
    "# Sauvegarder un export \"brut\" pour tra√ßabilit√©\n",
    "df_raw = pd.DataFrame(all_rows)\n",
    "df_raw.to_csv(RAW_DIR / \"accidents_sample_raw.csv\", index=False)\n",
    "print(f\"{len(df_raw)} lignes enregistr√©es\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c37d190",
   "metadata": {},
   "source": [
    "### Chargement des donn√©es sources\n",
    "\n",
    "Jusqu‚Äôici, nous avons vu comment interagir avec l‚ÄôAPI publique d‚ÄôOpendatasoft pour r√©cup√©rer les donn√©es dont nous avons besoin.  \n",
    "Un fichier d‚Äôexemple de 1 000 lignes a permis d‚Äôillustrer le principe de pagination et de test de l‚ÄôAPI.\n",
    "\n",
    "Cependant, l‚ÄôAPI limite les extractions √† des paquets de 100 enregistrements, et le jeu complet (plus de 500 000 lignes) aurait demand√© un temps de traitement trop important.  \n",
    "\n",
    "Pour la suite du brief, nous utiliserons donc directement le fichier CSV complet, d√©j√† t√©l√©charg√© et pr√©par√© √† partir de la source officielle :  \n",
    "üëâ [Accidents corporels de la circulation mill√©sim√© ‚Äî Opendatasoft](https://public.opendatasoft.com/explore/assets/accidents-corporels-de-la-circulation-millesime/export/)\n",
    "\n",
    "Ce fichier servira de base √† toutes les √©tapes suivantes de notre pipeline (nettoyage, transformation et analyse)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3c7b2b",
   "metadata": {},
   "source": [
    "### Nettoyage du jeu de donn√©es brut\n",
    "# Ce bloc charge le fichier CSV brut t√©l√©charg√© depuis Opendatasoft,\n",
    "# garde uniquement les colonnes utiles √† notre mod√®le, puis √©crit un CSV nettoy√© dans `data/raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a57da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../data/raw/accidents-corporels-de-la-circulation-millesime.csv\",\n",
    "                     sep=\";\", dtype=str, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "print(\"Colonnes d√©tect√©es:\", list(df.columns)[:8], \"...\")\n",
    "\n",
    "# 2) Garder seulement les colonnes utiles (sans renommage)\n",
    "cols = [\n",
    "    \"Identifiant de l'accident\",\"Sexe\",\"Date et heure\",\"Commune\",\"Ann√©e\",\"Mois\",\"Jour\",\"Heure minute\",\n",
    "    \"Lumi√®re\",\"Localisation\",\"Intersection\",\"Conditions atmosph√©riques\",\"Collision\",\n",
    "    \"D√©partement\",\"Code commune\",\"Code Insee\",\"Adresse\",\"Latitude\",\"Longitude\",\n",
    "    \"Surface\",\"Circulation\",\"Nombre de voies\",\"Cat√©gorie route\",\"Plan\",\"Profil\",\n",
    "    \"Infrastructure\",\"Situation\",\"Gps\",\"year_georef\",\n",
    "    \"Nom Officiel D√©partement\",\"Nom Officiel R√©gion\",\"Nom Officiel EPCI\",\"Nom Officiel Commune\",\n",
    "]\n",
    "df_filtered = df[cols]\n",
    "\n",
    "out_path = \"../data/raw/accidents-corporels-de-la-circulation-millesime_raw.csv\"\n",
    "df_filtered.to_csv(out_path, sep=\";\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"‚úÖ CSV cr√©√© (compatible Excel) :\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ee4456",
   "metadata": {},
   "source": [
    "### Nettoyage de base\n",
    "\n",
    "Nous allons maintenant proc√©der √† un premier nettoyage des donn√©es.  \n",
    "L‚Äôobjectif ici est d‚Äôobtenir un fichier exploitable et coh√©rent avant d‚Äôentamer les transformations plus avanc√©es.\n",
    "\n",
    "Ce nettoyage de base consiste √† :\n",
    "- supprimer les doublons √©ventuels,\n",
    "- √©liminer les lignes sans identifiant d‚Äôaccident,\n",
    "- pr√©parer un fichier CSV propre dans le dossier `data/cleaned`.\n",
    "\n",
    "Cette √©tape garantit que les traitements suivants (analyse, enrichissement, agr√©gations) reposeront sur un jeu de donn√©es unique et fiable.\n",
    "\n",
    "\n",
    "A PAUFINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0834ff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Charger le fichier d√©j√† filtr√©\n",
    "df_raw = pd.read_csv(\n",
    "    \"../data/raw/accidents-corporels-de-la-circulation-millesime_raw.csv\",\n",
    "    sep=\";\",\n",
    "    dtype=str,\n",
    "    encoding=\"utf-8-sig\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Fichier charg√© :\", len(df_raw), \"lignes\")\n",
    "print(\"Colonnes :\", list(df_raw.columns)[:8], \"...\")\n",
    "\n",
    "# R√®gles minimales : garder une cl√©, d√©doublonner\n",
    "df_clean = df_raw.copy()\n",
    "\n",
    "# supprimer les lignes sans identifiant d'accident\n",
    "# le nom exact de la colonne est \"Identifiant de l'accident\"\n",
    "if \"Identifiant de l'accident\" in df_clean.columns:\n",
    "    df_clean = (\n",
    "        df_clean.dropna(subset=[\"Identifiant de l'accident\"])\n",
    "                .drop_duplicates(subset=[\"Identifiant de l'accident\"])\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Apr√®s nettoyage :\", len(df_clean), \"lignes\")\n",
    "\n",
    "# Sauvegarde du jeu nettoy√©\n",
    "CLEAN_DIR = Path(\"../data/cleaned\")\n",
    "CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "clean_csv = CLEAN_DIR / \"accidents_clean.csv\"\n",
    "df_clean.to_csv(clean_csv, sep=\";\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"üíæ Fichier nettoy√© enregistr√© :\", clean_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f10741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"üß™ V√©rifications basiques\")\n",
    "\n",
    "# 1Ô∏è‚É£ V√©rifier les doublons sur la cl√©\n",
    "duplicates = df_clean[\"Identifiant de l'accident\"].duplicated().sum()\n",
    "print(f\"üîπ Doublons sur Identifiant de l'accident : {duplicates}\")\n",
    "\n",
    "# 2Ô∏è‚É£ V√©rifier les valeurs manquantes globales\n",
    "missing = df_clean.isna().mean().sort_values(ascending=False)\n",
    "print(\"\\nüîπ Colonnes avec valeurs manquantes (top 10) :\")\n",
    "print(missing.head(10).round(3))\n",
    "\n",
    "# 3Ô∏è‚É£ V√©rifier la coh√©rence des dates\n",
    "if \"Date et heure\" in df_clean.columns:\n",
    "    df_clean[\"Date et heure\"] = pd.to_datetime(\n",
    "        df_clean[\"Date et heure\"], errors=\"coerce\", utc=True\n",
    "    )\n",
    "    bad_dates = df_clean[\"Date et heure\"].isna().sum()\n",
    "    print(f\"\\nüîπ Dates invalides : {bad_dates}\")\n",
    "\n",
    "# 4Ô∏è‚É£ V√©rifier la coh√©rence g√©ographique\n",
    "if {\"Latitude\", \"Longitude\"} <= set(df_clean.columns):\n",
    "    df_clean[\"Latitude\"] = pd.to_numeric(df_clean[\"Latitude\"], errors=\"coerce\")\n",
    "    df_clean[\"Longitude\"] = pd.to_numeric(df_clean[\"Longitude\"], errors=\"coerce\")\n",
    "    geo_invalid = df_clean[\n",
    "        (df_clean[\"Latitude\"].isna()) | (df_clean[\"Longitude\"].isna())\n",
    "    ].shape[0]\n",
    "    print(f\"üîπ Coordonn√©es invalides ou manquantes : {geo_invalid}\")\n",
    "\n",
    "# 5Ô∏è‚É£ Aper√ßu des valeurs distinctes (cat√©gorielles)\n",
    "for col in [\"Lumi√®re\", \"Conditions atmosph√©riques\", \"Cat√©gorie route\"]:\n",
    "    if col in df_clean.columns:\n",
    "        uniques = df_clean[col].dropna().unique()\n",
    "        print(f\"\\nüîπ {col} ({len(uniques)} valeurs uniques):\")\n",
    "        print(uniques[:10])\n",
    "\n",
    "# 6Ô∏è‚É£ Aper√ßu al√©atoire\n",
    "print(\"\\nüîπ Exemple de lignes :\")\n",
    "display(df_clean.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78913c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRONZE_DIR = Path(\"../data/bronze\")\n",
    "BRONZE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "bronze_csv = BRONZE_DIR / \"accidents_bronze.csv\"\n",
    "\n",
    "df_clean.to_csv(bronze_csv, sep=\";\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"üíæ Donn√©es bronze enregistr√©es :\", bronze_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0a8125",
   "metadata": {},
   "source": [
    "## Prochaines √©tapes \n",
    "\n",
    "- G√©n√©rer `dim_time` √† partir des dates (ou d‚Äôun calendrier)  \n",
    "- Cr√©er `dim_location` (distinct de `codeinsee`, `departement`, lat/lon si dispo)  \n",
    "- Cr√©er `dim_conditions` (distinct de `lumiere`, `meteo`, `type_route`, `type_de_collision`)  \n",
    "- Mapper les IDs (cl√© naturelle ‚Üí cl√© de substitution)  \n",
    "- Remplir `fact_accident` avec les mesures (`ttue`, `tbg`, `tbl`, `tindm`, `grav`)  \n",
    "- √âcrire `sql/schema.sql` et (option) charger via SQLAlchemy dans SQLite/MySQL/PostgreSQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f851304",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b85fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Cr√©ation et test de connexion PostgreSQL locale\n",
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Configuration\n",
    "PG_HOST = \"localhost\"\n",
    "PG_PORT = 5432\n",
    "PG_SU_USER = \"postgres\"      # superuser par d√©faut (ou ton user admin)\n",
    "PG_SU_PASS = \"postgres\"      # mot de passe du superuser\n",
    "PG_DB = \"accidents\"\n",
    "PG_USER = \"accidents\"\n",
    "PG_PASS = \"accidents\"\n",
    "\n",
    "# Connexion au superuser (base postgres)\n",
    "url_su = f\"postgresql+psycopg2://{PG_SU_USER}:{PG_SU_PASS}@{PG_HOST}:{PG_PORT}/postgres\"\n",
    "engine_su = create_engine(url_su, future=True)\n",
    "\n",
    "# Cr√©ation base + user si absents\n",
    "with engine_su.begin() as conn:\n",
    "    conn.execute(text(f\"\"\"\n",
    "    DO $$\n",
    "    BEGIN\n",
    "        IF NOT EXISTS (SELECT FROM pg_roles WHERE rolname = '{PG_USER}') THEN\n",
    "            CREATE ROLE {PG_USER} LOGIN PASSWORD '{PG_PASS}';\n",
    "        END IF;\n",
    "        IF NOT EXISTS (SELECT FROM pg_database WHERE datname = '{PG_DB}') THEN\n",
    "            CREATE DATABASE {PG_DB} OWNER {PG_USER};\n",
    "        END IF;\n",
    "    END$$;\n",
    "    \"\"\"))\n",
    "print(\"‚úÖ Base et utilisateur v√©rifi√©s/cr√©√©s avec succ√®s\")\n",
    "\n",
    "# Connexion √† la nouvelle base\n",
    "url_app = f\"postgresql+psycopg2://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{PG_DB}\"\n",
    "engine_app = create_engine(url_app, future=True)\n",
    "\n",
    "# Test simple : cr√©ation d‚Äôune table temporaire\n",
    "with engine_app.begin() as conn:\n",
    "    conn.execute(text(\"CREATE TABLE IF NOT EXISTS test_conn (id SERIAL PRIMARY KEY, message TEXT);\"))\n",
    "    conn.execute(text(\"INSERT INTO test_conn (message) VALUES ('Connexion PostgreSQL OK');\"))\n",
    "    msg = conn.execute(text(\"SELECT message FROM test_conn ORDER BY id DESC LIMIT 1\")).scalar_one()\n",
    "\n",
    "print(\"üéØ Connexion test r√©ussie ‚Äî\", msg)\n",
    "## 1) Cr√©ation (si n√©cessaire) de la base de donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad462f1d",
   "metadata": {},
   "source": [
    "## 1) Cr√©ation (si n√©cessaire) de la base de donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29138420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichiers trouv√©s: []\n",
      "No SQL files provided.\n",
      "‚úÖ DDL ex√©cut√©s (si pr√©sents).\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "sql_dir = Path(\"../sql/bronze\").resolve()\n",
    "sql_files = sorted(sql_dir.glob(\"*.sql\"))\n",
    "print(\"Fichiers trouv√©s:\", [p.name for p in sql_files])\n",
    "\n",
    "db_utils.run_sql_files(sql_files)\n",
    "print(\"‚úÖ DDL ex√©cut√©s (si pr√©sents).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a03c175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: ‚úÖ Base 'roadsafety' d√©j√† existante.\n"
     ]
    }
   ],
   "source": [
    "created, msg = db_utils.ensure_database_exists()\n",
    "print(\"Status:\", msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62725da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c:\\Users\\khagr\\Desktop\\Formation\\projet\\etl-road-safety\\data\\db_utils.py\n",
    "import os\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "def get_cfg():\n",
    "    return {\n",
    "        \"PG_USER\": os.getenv(\"PG_USER\", \"postgres\"),\n",
    "        \"PG_PASS\": os.getenv(\"PG_PASS\", \"mot_de_passe_avec_√©?@!\"),\n",
    "        \"PG_HOST\": os.getenv(\"PG_HOST\", \"localhost\"),\n",
    "        \"PG_PORT\": int(os.getenv(\"PG_PORT\", \"5432\")),\n",
    "        \"PG_DB\":   os.getenv(\"PG_DB\", \"accident\"),\n",
    "        \"PG_SU_USER\": os.getenv(\"PG_SU_USER\"),  # optionnel\n",
    "        \"PG_SU_PASS\": os.getenv(\"PG_SU_PASS\"),  # optionnel\n",
    "    }\n",
    "\n",
    "def _kw(cfg):\n",
    "    # Connexion directe psycopg2, pas d'URL => pas d'UTF-8 √† d√©coder\n",
    "    return dict(\n",
    "        user=cfg[\"PG_USER\"],\n",
    "        password=cfg[\"PG_PASS\"],\n",
    "        host=cfg[\"PG_HOST\"],\n",
    "        port=cfg[\"PG_PORT\"],\n",
    "        dbname=cfg[\"PG_DB\"],\n",
    "        options=\"-c client_encoding=UTF8\",\n",
    "    )\n",
    "\n",
    "def _kw_super(cfg):\n",
    "    user = cfg[\"PG_SU_USER\"] or cfg[\"PG_USER\"]\n",
    "    pwd  = cfg[\"PG_SU_PASS\"] or cfg[\"PG_PASS\"]\n",
    "    return dict(\n",
    "        user=user,\n",
    "        password=pwd,\n",
    "        host=cfg[\"PG_HOST\"],\n",
    "        port=cfg[\"PG_PORT\"],\n",
    "        dbname=\"postgres\",\n",
    "        options=\"-c client_encoding=UTF8\",\n",
    "    )\n",
    "\n",
    "def get_engine():\n",
    "    import psycopg2\n",
    "    cfg = get_cfg()\n",
    "    def _creator():\n",
    "        return psycopg2.connect(**_kw(cfg))\n",
    "    # IMPORTANT : on passe un creator ‚Üí **aucune URL**\n",
    "    return create_engine(\"postgresql+psycopg2://\", creator=_creator, future=True)\n",
    "\n",
    "def ensure_database_exists():\n",
    "    import psycopg2\n",
    "    cfg = get_cfg()\n",
    "    dbname = cfg[\"PG_DB\"]\n",
    "\n",
    "    def _creator_su():\n",
    "        return psycopg2.connect(**_kw_super(cfg))\n",
    "\n",
    "    engine_su = create_engine(\"postgresql+psycopg2://\", creator=_creator_su, future=True)\n",
    "\n",
    "    with engine_su.begin() as conn:\n",
    "        exists = conn.execute(\n",
    "            text(\"SELECT 1 FROM pg_database WHERE datname = :name\"),\n",
    "            {\"name\": dbname}\n",
    "        ).fetchone()\n",
    "        if exists:\n",
    "            return False, f\"‚úÖ Base '{dbname}' d√©j√† existante.\"\n",
    "        conn.execute(text(f'CREATE DATABASE \"{dbname}\"'))\n",
    "        return True, f\"üÜï Base '{dbname}' cr√©√©e avec succ√®s.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0a6685",
   "metadata": {},
   "source": [
    "## 1) Cr√©ation (si n√©cessaire) de la base de donn√©es"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
