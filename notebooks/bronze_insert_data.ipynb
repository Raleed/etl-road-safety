{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b1d096c",
   "metadata": {},
   "source": [
    "# ETL — Accidents de la route\n",
    "\n",
    "Ce notebook montre, pas à pas, comment :\n",
    "1) se connecter à l’API publique Opendatasoft,\n",
    "2) récupérer un petit échantillon,\n",
    "3) paginer pour extraire un volume plus grand,\n",
    "4) sauvegarder les données brutes en CSV,\n",
    "5) poser les bases du nettoyage (à faire en équipe).\n",
    "\n",
    "> **Pourquoi ce format ?**  \n",
    "> Un notebook est idéal pour apprendre : on alterne **explications** (Markdown) et **code** (Python), et on voit les résultats immédiatement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03f38a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "CLEAN_DIR = Path(\"../data/cleaned\")\n",
    "RAW_DIR = Path(\"../data/raw\")\n",
    "CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "808a699c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes détectées: [\"Identifiant de l'accident\", 'Date et heure', 'Commune', 'Année', 'Mois', 'Jour', 'Heure minute', 'Lumière'] ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_name = \"accidents-corporels-de-la-circulation-millesime.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(RAW_DIR / csv_name,\n",
    "                     sep=\";\", dtype=str, encoding=\"utf-8-sig\")\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(RAW_DIR / csv_name,\n",
    "                     sep=\";\", dtype=str, encoding=\"latin1\")\n",
    "\n",
    "print(\"Colonnes détectées:\", list(df.columns)[:8], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "987fa4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['identifiant_de_l_accident',\n",
       " 'date_et_heure',\n",
       " 'commune',\n",
       " 'annee',\n",
       " 'mois',\n",
       " 'jour',\n",
       " 'heure_minute',\n",
       " 'lumiere',\n",
       " 'localisation',\n",
       " 'intersection',\n",
       " 'conditions_atmospheriques',\n",
       " 'collision',\n",
       " 'departement',\n",
       " 'code_commune',\n",
       " 'code_insee',\n",
       " 'adresse',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'code_postal',\n",
       " 'numero',\n",
       " 'coordonnees',\n",
       " 'pr',\n",
       " 'surface',\n",
       " 'v1',\n",
       " 'circulation',\n",
       " 'voie_reservee',\n",
       " 'env1',\n",
       " 'voie',\n",
       " 'largeur_de_la_chaussee',\n",
       " 'v2',\n",
       " 'largeur_terre_plein_central',\n",
       " 'nombre_de_voies',\n",
       " 'categorie_route',\n",
       " 'pr1',\n",
       " 'plan',\n",
       " 'profil',\n",
       " 'infrastructure',\n",
       " 'situation',\n",
       " 'annee_de_naissance',\n",
       " 'sexe',\n",
       " 'action_pieton',\n",
       " 'gravite',\n",
       " 'existence_equipement_de_securite',\n",
       " 'utilisation_equipement_de_securite',\n",
       " 'localisation_du_pieton',\n",
       " 'identifiant_vehicule',\n",
       " 'place',\n",
       " 'categorie_d_usager',\n",
       " 'pieton_seul_ou_non',\n",
       " 'motif_trajet',\n",
       " 'point_de_choc',\n",
       " 'man_uvre',\n",
       " 'sens',\n",
       " 'obstacle_mobile_heurte',\n",
       " 'obstacle_fixe_heurte',\n",
       " 'categorie_vehicule',\n",
       " 'nombre_d_occupants',\n",
       " 'gps',\n",
       " 'date',\n",
       " 'year_georef',\n",
       " 'nom_officiel_commune',\n",
       " 'code_officiel_departement',\n",
       " 'nom_officiel_departement',\n",
       " 'code_officiel_epci',\n",
       " 'nom_officiel_epci',\n",
       " 'code_officiel_region',\n",
       " 'nom_officiel_region',\n",
       " 'nom_officiel_commune_arrondissement_municipal',\n",
       " 'code_officiel_commune']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_columns(df, inplace=False):\n",
    "    \"\"\"\n",
    "    Normalise les noms de colonnes d'un DataFrame en snake_case sans accents ni caractères spéciaux.\n",
    "    Règles :\n",
    "      - retire les accents\n",
    "      - met en minuscules\n",
    "      - remplace tout caractère non alphanumérique par un underscore\n",
    "      - réduit les underscores multiples en un seul\n",
    "      - supprime les underscores en début/fin\n",
    "      - si le nom commence par un chiffre, préfixe par 'c_'\n",
    "      - si le résultat est vide, remplace par 'unknown'\n",
    "      - garantit l'unicité des noms en ajoutant des suffixes _2, _3, ...\n",
    "    Arguments :\n",
    "      df : pandas.DataFrame\n",
    "      inplace : bool (False par défaut). Si True, renomme les colonnes sur place et retourne le même objet.\n",
    "    Retour :\n",
    "      pandas.DataFrame avec colonnes normalisées.\n",
    "    \"\"\"\n",
    "    import unicodedata\n",
    "    import re\n",
    "    import pandas as pd\n",
    "\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "\n",
    "    def _slug(name: object) -> str:\n",
    "        s = \"\" if name is None else str(name)\n",
    "        # Normaliser unicode et séparer les accents\n",
    "        s = unicodedata.normalize(\"NFKD\", s)\n",
    "        # Enlever les caractères de composition (accents)\n",
    "        s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "        s = s.lower()\n",
    "        # Remplacer tout caractère non alphanumérique par underscore\n",
    "        s = re.sub(r\"[^a-z0-9]+\", \"_\", s)\n",
    "        # Réduire underscores multiples et trim\n",
    "        s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "        # Préfixer si commence par chiffre\n",
    "        if re.match(r\"^[0-9]\", s):\n",
    "            s = \"c_\" + s\n",
    "        if s == \"\":\n",
    "            s = \"unknown\"\n",
    "        return s\n",
    "\n",
    "    # Appliquer la normalisation\n",
    "    orig_cols = list(df.columns)\n",
    "    normalized = [_slug(c) for c in orig_cols]\n",
    "\n",
    "    # Garantir l'unicité des noms\n",
    "    seen = {}\n",
    "    unique_cols = []\n",
    "    for name in normalized:\n",
    "        base = name\n",
    "        if name not in seen:\n",
    "            seen[name] = 1\n",
    "            unique_cols.append(name)\n",
    "        else:\n",
    "            seen[name] += 1\n",
    "            new_name = f\"{base}_{seen[name]}\"\n",
    "            # garantir que new_name lui-même n'existe pas déjà\n",
    "            while new_name in seen:\n",
    "                seen[base] += 1\n",
    "                new_name = f\"{base}_{seen[base]}\"\n",
    "            seen[new_name] = 1\n",
    "            unique_cols.append(new_name)\n",
    "\n",
    "    # Renommer le DataFrame\n",
    "    mapping = dict(zip(orig_cols, unique_cols))\n",
    "    df = df.rename(columns=mapping)\n",
    "\n",
    "    return df\n",
    "df_correct_col_name = normalize_columns(df, inplace=True)\n",
    "df_correct_col_name.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20c15e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utilitaires de connexion et d'insertion pour Postgres ; schemas bronze\n",
    "\n",
    "Dépendances :\n",
    "  - sqlalchemy\n",
    "  - psycopg2-binary\n",
    "  - pandas\n",
    "\n",
    "Fonctions exposées :\n",
    "  - get_engine(db_url) -> sqlalchemy.Engine\n",
    "  - insert_df_to_table(engine, df, schema, table, table_columns=None, batch_size=1000)\n",
    "\n",
    "Exemple d'utilisation :\n",
    "  engine = get_engine(\"postgresql+psycopg2://user:pass@host:5432/dbname\")\n",
    "  inserted = insert_df_to_table(engine, df_normalized, \"bronze\", \"caracteristiques_raw\",\n",
    "                                table_columns=[\"identifiant_de_l_accident\", \"date_et_heure\", ...])\n",
    "\"\"\"\n",
    "from typing import List, Optional\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine import Engine\n",
    "from psycopg2.extras import execute_values\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_engine(db_url: str) -> Engine:\n",
    "    \"\"\"\n",
    "    Retourne un SQLAlchemy Engine pour l'URL \n",
    "    postgresql+psycopg2://user:password@host:port/dbname\n",
    "    \"\"\"\n",
    "    return create_engine(db_url, client_encoding=\"utf8\")\n",
    "\n",
    "\n",
    "def insert_df_to_table(\n",
    "    engine: Engine,\n",
    "    df: pd.DataFrame,\n",
    "    schema: str,\n",
    "    table: str,\n",
    "    table_columns: Optional[List[str]] = None,\n",
    "    batch_size: int = 1000,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Insère un DataFrame dans une table Postgres via psycopg2.execute_values (performant pour gros volumes).\n",
    "\n",
    "    Comportement :\n",
    "      - Si table_columns est fourni : on utilise cet ordre de colonnes pour l'insertion.\n",
    "        * Les colonnes absentes dans df sont créées et remplies par NULL.\n",
    "        * Les colonnes supplémentaires dans df sont ignorées.\n",
    "      - Si table_columns est None : on utilise l'ordre des colonnes présentes dans df.\n",
    "      - Retourne le nombre de lignes insérées.\n",
    "      - Gère commit/rollback automatiquement.\n",
    "\n",
    "    Arguments :\n",
    "      engine        : SQLAlchemy Engine (obtenu via get_engine)\n",
    "      df            : pandas.DataFrame (les colonnes doivent déjà être normalisées)\n",
    "      schema        : schéma SQL (ex. \"bronze\")\n",
    "      table         : nom de la table (ex. \"caracteristiques_raw\")\n",
    "      table_columns : liste ordonnée des colonnes à insérer (optionnel)\n",
    "      batch_size    : taille de page pour execute_values (par défaut 1000)\n",
    "\n",
    "    Remarques :\n",
    "      - Nécessite psycopg2 (sqlalchemy devra utiliser l'adaptateur psycopg2).\n",
    "      - Tous types Python usuels sont supportés (str, int, float, None, bool, datetime, ...).\n",
    "    \"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        return 0\n",
    "\n",
    "    # Déterminer colonnes cibles et préparer df_subset avec la bonne colonne ordre\n",
    "    if table_columns is None:\n",
    "        cols = list(df.columns)\n",
    "    else:\n",
    "        cols = list(table_columns)\n",
    "        # ajouter colonnes manquantes dans df en les remplissant avec None\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                df[c] = None\n",
    "\n",
    "    # Conserver uniquement les colonnes cibles\n",
    "    df_subset = df[cols].copy()\n",
    "\n",
    "    # Convertir les NaN pandas en None pour psycopg2\n",
    "    df_subset = df_subset.where(pd.notnull(df_subset), None)\n",
    "\n",
    "    # Préparer les tuples de valeurs\n",
    "    records = [tuple(x) for x in df_subset.itertuples(index=False, name=None)]\n",
    "    if not records:\n",
    "        return 0\n",
    "\n",
    "    # Construire la clause des colonnes (avec guillemets pour noms contenant underscore ou majuscules)\n",
    "    cols_sql = \", \".join([f'\"{c}\"' for c in cols])\n",
    "    insert_sql = f'INSERT INTO \"{schema}\".\"{table}\" ({cols_sql}) VALUES %s'\n",
    "\n",
    "    # Obtenir une connexion psycopg2 à partir de l'engine SQLAlchemy\n",
    "    conn = engine.raw_connection()\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        # execute_values fait des inserts par batch très efficaces\n",
    "        execute_values(cur, insert_sql, records, page_size=batch_size)\n",
    "        conn.commit()\n",
    "        return len(records)\n",
    "    except Exception:\n",
    "        conn.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea14eda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "475911"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Engine = get_engine(\"postgresql+psycopg2://postgres:postgres2025%40@localhost:54785/db_accident\")\n",
    "table_colonnes = [\n",
    "    \"identifiant_de_l_accident\",\n",
    "    \"date_et_heure\",\n",
    "    \"commune\",\n",
    "    \"annee\",\n",
    "    \"mois\",\n",
    "    \"jour\",\n",
    "    \"heure_minute\",\n",
    "    \"lumiere\",\n",
    "    \"localisation\",\n",
    "    \"intersection\",\n",
    "    \"conditions_atmospheriques\",\n",
    "    \"collision\",\n",
    "    \"departement\",\n",
    "    \"code_commune\",\n",
    "    \"code_insee\",\n",
    "    \"adresse\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"code_postal\",\n",
    "    \"numero\",\n",
    "    \"coordonnees\",\n",
    "    \"pr\",\n",
    "    \"surface\",\n",
    "    \"v1\",\n",
    "    \"circulation\",\n",
    "    \"voie_reservee\",\n",
    "    \"env1\",\n",
    "    \"voie\",\n",
    "    \"largeur_de_la_chaussee\",\n",
    "    \"v2\",\n",
    "    \"largeur_terre_plein_central\",\n",
    "    \"nombre_de_voies\",\n",
    "    \"categorie_route\",\n",
    "    \"pr1\",\n",
    "    \"plan\",\n",
    "    \"profil\",\n",
    "    \"infrastructure\",\n",
    "    \"situation\",\n",
    "    \"gps\",\n",
    "    \"date\",\n",
    "    \"year_georef\",\n",
    "    \"nom_officiel_commune\",\n",
    "    \"code_officiel_departement\",\n",
    "    \"nom_officiel_departement\",\n",
    "    \"code_officiel_epci\",\n",
    "    \"nom_officiel_epci\",\n",
    "    \"code_officiel_region\",\n",
    "    \"nom_officiel_region\",\n",
    "    \"nom_officiel_commune_arrondissement_municipal\",\n",
    "    \"code_officiel_commune\"\n",
    "]\n",
    "df_filtred_to_insert = df_correct_col_name[table_colonnes].copy()\n",
    "insert_df_to_table(Engine, df_filtred_to_insert, \"bronze\", \"caracteristiques_raw\",\n",
    "                   table_columns=table_colonnes, batch_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a047ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "475911"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_colonnes = [\n",
    "    \"identifiant_de_l_accident\",\n",
    "    \"categorie_route\",\n",
    "    \"voie\",\n",
    "    \"v1\",\n",
    "    \"v2\",\n",
    "    \"circulation\",\n",
    "    \"nombre_de_voies\",\n",
    "    \"voie_reservee\",\n",
    "    \"profil\",\n",
    "    \"pr\",\n",
    "    \"pr1\",\n",
    "    \"plan\",\n",
    "    \"largeur_terre_plein_central\",\n",
    "    \"largeur_de_la_chaussee\",\n",
    "    \"surface\",\n",
    "    \"infrastructure\",\n",
    "    \"situation\",\n",
    "    \"env1\",\n",
    "    \"coordonnees\"\n",
    "]\n",
    "df_filtred_to_insert = df_correct_col_name[table_colonnes].copy()\n",
    "insert_df_to_table(Engine, df_filtred_to_insert, \"bronze\", \"lieux_raw\",\n",
    "                   table_columns=table_colonnes, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80da58aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "475911"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changer le nom du colonne \n",
    "df_correct_col_name.rename(\n",
    "    columns={\n",
    "        'man_uvre': 'manoeuvre',\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "table_colonnes = [\n",
    "    \"identifiant_de_l_accident\",\n",
    "    \"identifiant_vehicule\",\n",
    "    \"sens\",\n",
    "    \"categorie_vehicule\",\n",
    "    \"obstacle_fixe_heurte\",\n",
    "    \"obstacle_mobile_heurte\",\n",
    "    \"point_de_choc\",\n",
    "    \"manoeuvre\",\n",
    "    \"nombre_d_occupants\"\n",
    "]\n",
    "\n",
    "df_filtred_to_insert = df_correct_col_name[table_colonnes].copy()\n",
    "insert_df_to_table(Engine, df_filtred_to_insert, \"bronze\", \"vehicules_raw\",\n",
    "                   table_columns=table_colonnes, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe16515c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "475911"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_colonnes = [\n",
    "    \"identifiant_de_l_accident\",\n",
    "    \"identifiant_vehicule\",\n",
    "    \"place\",\n",
    "    \"categorie_d_usager\",   \n",
    "    \"gravite\",\n",
    "    \"sexe\",\n",
    "    \"annee_de_naissance\",\n",
    "    \"motif_trajet\",                         \n",
    "    \"existence_equipement_de_securite\",\n",
    "    \"utilisation_equipement_de_securite\",  \n",
    "    \"localisation_du_pieton\",            \n",
    "    \"action_pieton\",                       \n",
    "    \"pieton_seul_ou_non\"\n",
    "]\n",
    "df_filtred_to_insert = df_correct_col_name[table_colonnes].copy()\n",
    "insert_df_to_table(Engine, df_filtred_to_insert, \"bronze\", \"usagers_raw\",\n",
    "                   table_columns=table_colonnes, batch_size=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
